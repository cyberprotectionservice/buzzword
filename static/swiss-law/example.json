{
  "intro": "### *A sample investigation*\n\nThe corpus being investigated is currently comprised of three books, which have undergone optical character recognition (OCR) via [tesseract](https://opensource.google/projects/tesseract) and parsing (segmenting the words in the texts and adding grammatical information) via [buzz](https://github.com/interrogator/buzz)/[spaCy](https://spacy.io).\n\nBecause the original texts are typset with Fraktur fonts, and because the books themselves are not in ideal condition, accuracy of the OCR process far from perfect. Incorrect OCR analysis in turn effects parser accuracy, and thus, it can be expected that the accuracy of any computational analysis of the raw OCR output will also suffer.\n\nTo address this problem, we built the [compare interface](/compare), which allows registered users to edit the results of the OCR process, and also to add annotations in the form of XML tags, which can later be used in the analysis process. Changes made to the OCR are not instantly reflected in the analysis interface; this update is performed manually, following review of submitted corrections.\n\nThe demo investigation presented on this page will therefore improve in accuracy when re-run on increasingly correct versions of the OCR text.\n\nAs can be seen via the [explore](/explore) interface, the corpus contains nearly 300,000 tokens, though many of these are misanalysed by the OCR engine. Of these, we will focus on tokens analysed as nouns by the *spaCy* parser, looking at their frequencies in each text and using concordancing to understand how the words are used in context. This is therefore an exercise in *distant reading*; without reading the texts cover-to-cover, we can locate central themes and learn how these themes are typically represented in the text.\n\nTo locate nouns in the text, we perform two searches. First, we try to remove non-word tokens (i.e. OCR errors) by searching for tokens matching a [regular expression](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285), `[A-Za-z]{3,}`. This means, match words that contain three or more consecutive alphabetical characters, either in upper or lowercase. In the explore view, on the *Dataset* tab, this would look like:",
   "intro2": "Searching, in *buzzword*, can be done in iterations. Rather than creating one complicated query that matches exactly what you need, you can simply perform multiple searches, drilling down to get what you want. So, after removing non-words, we then search again, within the first set of results, for tokens whose wordclass is analysed as *NOUN*. This second search would be written like so:",
  "freq": "## Word frequencies\n\nThe search process alone does give us a great deal of useful information about our corpus. What we need to do next is calculate the relative frequency of each noun, by each year represented in the corpus.\n\nTo do this in *buzzword*'s *Explore* interface, you would move to the *Frequencies* tab, selecting your most recent search. Here, there are four dropdown menus, from which you can decide how to calculate a meaningful result from the selected search.",
  "freq2": "Rather than calculate word frequencies, we can set *lemma* as the column, in order to collapse singular and plural forms of words into the same result. For the index, we use *Year*, corresponding to the publication year of each book in the corpus. Finally, we calculate relative frequencies, which provides a much more useful statistic than absolute frequency. Once we hit *Generate*, we are presented with a table like that to the left of this text.\n\nThe table shows us the frequencies of the most common nouns in the books, relative to all nouns in that particular text. You can use the dropdown menu above the table to view other wordclasses: try looking at common verbs (`VERB`), adjectives (`ADJ`) or proper nouns (`PROPN`).",
  "vis": "\n\n\n\n## Visualising frequencies\n\nWhatever has been generated in *buzzword*'s *Frequencies* view can be displayed as an interactive visualisation via [*plotly*](https://plotly.com/)/[*dash*](https://plotly.com/dash/). Visualisation of quantitative data is a critical step in the process of distant reading, allowing the researcher to quickly uncover patterns in the language of the corpus.\n\nThis plot uses the frequency data from the frequency table above. By default, you are looking at common nouns in the corpus, but you can use the menu above the frequency table to investigate any other wordclass instead.\n\nOn this page, as in *buzzword*'s *Chart* view, figures are interactive: you can zoom in on parts of the plot, use the Lasso to highlight interesting segments, and export results to `png` format to save on your hard disk. You can choose the plot type, number of items, and orientation, and regenerate the figure as many times as you like.",
  "conc": "## Concordancing\n\nConcordancing, sometimes called *Keyword in context (KWIC)*, allows you to quickly see each instance of a word, and what occurs alongside it. You can search for anything you like in the space below. For an example, here we have selected `Ordnung`, which appears in the list of most frequent nouns, especially in the 1803 text data. Try inserting other words from the frequency list above, to see how they behave in context. Note that the concordancer here is a simple one; for more advanced functionality, you should use the dedicated [explore interface](/explore).",
  "end": "## Where to from here?\n\nThat's as far as this sample investigation goes, for now. Hopefully, this is enough to show you how to construct a workflow using *buzzword* and the Swiss Digital Law Discovery Corpus. Good luck!"
}